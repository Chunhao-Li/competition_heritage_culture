{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../datasets/my_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_path = os.path.join(data_dir, 'schema.json')\n",
    "schema = open(schema_path, 'r')\n",
    "predicate_dict = {}\n",
    "ners = set()\n",
    "for l_idx, line in enumerate(schema):\n",
    "    tmp = json.loads(line)\n",
    "    predicate_dict[tmp['predicate']] = (tmp['subjtct_type'], tmp['object_type'])\n",
    "    ners.add(tmp['subjtct_type'])\n",
    "    ners.add(tmp['object_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'相关文物': ('遗址文化', '文物藏品'),\n",
       " '所在地': ('遗址文化/单位机构', '地点'),\n",
       " '所属时代': ('遗址文化/文物藏品/人物', '历史时代'),\n",
       " '文物尺寸': ('文物藏品', '尺寸'),\n",
       " '面积大小': ('遗址文化/单位机构', '面积'),\n",
       " '发现时间': ('遗址文化/文物藏品', '时间'),\n",
       " '保存于': ('文物藏品', '单位机构'),\n",
       " '建设时间': ('遗址文化', '时间'),\n",
       " '文保级别': ('遗址文化', '保护级别'),\n",
       " '相关建筑': ('遗址文化', '建筑'),\n",
       " '相关人物': ('遗址文化/文物藏品/人物', '人物'),\n",
       " '相关单位': ('遗址文化', '单位机构'),\n",
       " '馆藏物品': ('单位机构', '文物藏品'),\n",
       " '文化类型': ('遗址文化', '文化'),\n",
       " '成立时间': ('单位机构', '时间'),\n",
       " '文化组成': ('遗址文化', '遗址文化'),\n",
       " '相关文献': ('遗址文化/文物藏品', '书籍文献'),\n",
       " '文物重量': ('文物藏品', '重量'),\n",
       " '馆长': ('单位机构', '人物')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(predicate_dict))\n",
    "predicate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['相关文物', '所在地', '所属时代', '文物尺寸', '面积大小', '发现时间', '保存于', '建设时间', '文保级别', '相关建筑', '相关人物', '相关单位', '馆藏物品', '文化类型', '成立时间', '文化组成', '相关文献', '文物重量', '馆长'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicate_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ners_set = set()\n",
    "for _, (n1, n2) in predicate_dict.items():\n",
    "    ners_set.add(n1)\n",
    "    ners_set.add(n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'书籍文献',\n",
       " '人物',\n",
       " '保护级别',\n",
       " '单位机构',\n",
       " '历史时代',\n",
       " '地点',\n",
       " '尺寸',\n",
       " '建筑',\n",
       " '文化',\n",
       " '文物藏品',\n",
       " '时间',\n",
       " '遗址文化',\n",
       " '遗址文化/单位机构',\n",
       " '遗址文化/文物藏品',\n",
       " '遗址文化/文物藏品/人物',\n",
       " '重量',\n",
       " '面积'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(ners_set))\n",
    "ners_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# 处理数据\n",
    "\n",
    "def get_data(file_path, key_word):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        docs = []\n",
    "        for l_idx, line in enumerate(file):\n",
    "            doc = {}\n",
    "            info = json.loads(line)\n",
    "            doc['doc_key'] = str(l_idx) + key_word\n",
    "            ori_sent = info['sentence']\n",
    "            if len(ori_sent) > 512:\n",
    "                # 超过最大长度，truncate\n",
    "                print(len(ori_sent))\n",
    "                ori_sent = ori_sent[:512]\n",
    "            doc['sentences'] = [[ch for ch in ori_sent]] #TODO split into tokens\n",
    "            doc['ner'] = [[]]\n",
    "            doc['relations'] = [[]]\n",
    "            if key_word == 'train':\n",
    "                doc['ner'] = [[]]\n",
    "                doc['relations'] = [[]]\n",
    "                spos = info['spos']\n",
    "                entity_set = set()\n",
    "                for cur_spo in spos:\n",
    "                    cur_rel = cur_spo['p']\n",
    "                     # 不在schema中 e.g. 馆长\n",
    "                    if cur_rel not in predicate_dict:\n",
    "                        continue\n",
    "                    s_ner, o_ner = predicate_dict[cur_rel]\n",
    "                    sub = cur_spo['s']\n",
    "                    obj = cur_spo['o']\n",
    "                    sub_start_idx = ori_sent.find(sub)\n",
    "                    sub_end_idx = sub_start_idx + len(sub) - 1\n",
    "                    obj_start_idx = ori_sent.find(obj)\n",
    "                    obj_end_idx = obj_start_idx + len(obj) - 1\n",
    "                    if sub_start_idx != -1:\n",
    "                        doc['ner'][0].append([sub_start_idx, sub_end_idx, s_ner])\n",
    "                    if obj_start_idx != -1:\n",
    "                        doc['ner'][0].append([obj_start_idx, obj_end_idx, o_ner])\n",
    "                    if sub_start_idx != -1 and obj_start_idx != -1:\n",
    "\n",
    "                        doc['relations'][0].append([sub_start_idx, sub_end_idx, obj_start_idx, obj_end_idx, cur_rel])\n",
    "\n",
    "\n",
    "            docs.append(doc)\n",
    "        return docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed_dir = \"../datasets/my_dataset_processed/\"\n",
    "train_path = os.path.join(data_dir, 'train.json')\n",
    "train_data = get_data(train_path, 'train')\n",
    "\n",
    "with open(os.path.join(data_processed_dir, 'train.json'),\"w\", encoding=\"utf-8\") as f:\n",
    "    for d in train_data:\n",
    "        f.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
    "\n",
    "eval_path = os.path.join(data_dir, 'eval.json')\n",
    "eval_data = get_data(eval_path, 'eval')\n",
    "\n",
    "with open(os.path.join(data_processed_dir, 'test.json'),\"w\", encoding=\"utf-8\") as f:\n",
    "    for d in eval_data:\n",
    "        f.write(json.dumps(d, ensure_ascii=False)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "742"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for doc in eval_data:\n",
    "    count += len(doc['relations'][0])\n",
    "count "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  baseline--UIE 参考"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 5) (200, 5)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "examples = []\n",
    "with open(\"../datasets/my_dataset/train.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for i in f.readlines():\n",
    "        examples.append(json.loads(i))\n",
    "train = pd.DataFrame(examples)\n",
    "train [\"idlength\"] = [len(i) for i in train [\"id\"]]\n",
    "train [\"slength\"] = [len(i) for i in train [\"sentence\"]]\n",
    "\n",
    "examples = []\n",
    "with open(\"../datasets/my_dataset/eval.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for i in f.readlines():\n",
    "        examples.append(json.loads(i))\n",
    "eval = pd.DataFrame(examples)\n",
    "eval[\"idlength\"] = [len(i) for i in eval[\"id\"]]\n",
    "eval[\"slength\"] = [len(i) for i in eval[\"sentence\"]]\n",
    "print(train.shape,eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<AxesSubplot:>, <AxesSubplot:>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"slength\"].hist(),eval[\"slength\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'subjtct_type': '遗址文化', 'predicate': '相关文物', 'object_type': '文物藏品'},\n",
       " {'subjtct_type': '遗址文化/单位机构', 'predicate': '所在地', 'object_type': '地点'},\n",
       " {'subjtct_type': '遗址文化/文物藏品/人物', 'predicate': '所属时代', 'object_type': '历史时代'},\n",
       " {'subjtct_type': '文物藏品', 'predicate': '文物尺寸', 'object_type': '尺寸'},\n",
       " {'subjtct_type': '遗址文化/单位机构', 'predicate': '面积大小', 'object_type': '面积'},\n",
       " {'subjtct_type': '遗址文化/文物藏品', 'predicate': '发现时间', 'object_type': '时间'},\n",
       " {'subjtct_type': '文物藏品', 'predicate': '保存于', 'object_type': '单位机构'},\n",
       " {'subjtct_type': '遗址文化', 'predicate': '建设时间', 'object_type': '时间'},\n",
       " {'subjtct_type': '遗址文化', 'predicate': '文保级别', 'object_type': '保护级别'},\n",
       " {'subjtct_type': '遗址文化', 'predicate': '相关建筑', 'object_type': '建筑'},\n",
       " {'subjtct_type': '遗址文化/文物藏品/人物', 'predicate': '相关人物', 'object_type': '人物'},\n",
       " {'subjtct_type': '遗址文化', 'predicate': '相关单位', 'object_type': '单位机构'},\n",
       " {'subjtct_type': '单位机构', 'predicate': '馆藏物品', 'object_type': '文物藏品'},\n",
       " {'subjtct_type': '遗址文化', 'predicate': '文化类型', 'object_type': '文化'},\n",
       " {'subjtct_type': '单位机构', 'predicate': '成立时间', 'object_type': '时间'},\n",
       " {'subjtct_type': '遗址文化', 'predicate': '文化组成', 'object_type': '遗址文化'},\n",
       " {'subjtct_type': '遗址文化/文物藏品', 'predicate': '相关文献', 'object_type': '书籍文献'},\n",
       " {'subjtct_type': '文物藏品', 'predicate': '文物重量', 'object_type': '重量'},\n",
       " {'subjtct_type': '单位机构', 'predicate': '馆长', 'object_type': '人物'}]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "schema_list = []\n",
    "with open(\"../datasets/my_dataset/schema.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for i in f.readlines():\n",
    "        schema_list.append(json.loads(i))\n",
    "schema_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uie_data(df):\n",
    "    prompt_datas = []\n",
    "    for idx,rows in df.iterrows():\n",
    "        text = rows[\"sentence\"]\n",
    "        spo_list = rows[\"spos\"]\n",
    "        entitys = []\n",
    "        relations = []\n",
    "        for spo in spo_list:\n",
    "            pso = [(schema[\"subjtct_type\"],schema[\"object_type\"])for schema in schema_list if schema[\"predicate\"] == spo['p']][0]\n",
    "            if spo['s'] not in entitys:\n",
    "                entitys.append([spo['s'],pso[0]])\n",
    "            if spo['o'] not in entitys:\n",
    "                entitys.append([spo['o'],pso[1]])\n",
    "            if spo['p'] not in relations:\n",
    "                relations.append([spo['p'],spo['s'],spo['o']])\n",
    "        prompt_data = {}\n",
    "        prompt_data[\"text\"] = rows[\"sentence\"]\n",
    "        prompt_data[\"entities\"] = []\n",
    "        prompt_data[\"relations\"] = []\n",
    "        id_number = 0\n",
    "        for entity in entitys:\n",
    "            entity_pre = {}\n",
    "            entity_pre['id'] = id_number\n",
    "            entity_pre[\"start_offset\"] = text.index(entity[0])\n",
    "            entity_pre[\"end_offset\"] = text.index(entity[0])+len(entity[0])\n",
    "            entity_pre[\"label\"] = entity[1]\n",
    "            entity_pre[\"name\"] = entity[0]\n",
    "            prompt_data[\"entities\"].append(entity_pre)\n",
    "            id_number += 1\n",
    "\n",
    "        for relation in relations:\n",
    "            relation_pre = {}\n",
    "            from_id = [e['id'] for e in prompt_data[\"entities\"] if e['name']==relation[1]][0]\n",
    "            to_id = [e['id'] for e in prompt_data[\"entities\"] if e['name']==relation[2]][0]\n",
    "            relation_pre[\"from_id\"] = from_id\n",
    "            relation_pre[\"to_id\"] = to_id\n",
    "            relation_pre[\"type\"] = relation[0]\n",
    "            prompt_data[\"relations\"].append(relation_pre)\n",
    "        prompt_datas.append(prompt_data)\n",
    "    return prompt_datas\n",
    "\n",
    "data = get_uie_data(train)\n",
    "dev = get_uie_data(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import json\n",
    "import os\n",
    "\n",
    "# NER预测文件处理\n",
    "processed_f = open('../datasets/my_dataset_processed/ner_pred_result.json', 'w', encoding='utf8')\n",
    "with open('./bert_models/PL-Marker-roberta-zh-10/ent_pred_test.json','r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        processed_f.write(json.dumps(json.loads(line), ensure_ascii=False)+'\\n')\n",
    "\n",
    "processed_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 后处理RE的结果\n",
    "def post_process(pred_path, input_path, output_path):\n",
    "    with open(pred_path, 'r', encoding='utf8') as pred_f:\n",
    "        for l in pred_f:\n",
    "            pred = json.loads(l)\n",
    "    outputs = []\n",
    "    with open(input_path, 'r', encoding='utf8') as input_f:\n",
    "        for i, line in enumerate(input_f):\n",
    "            line = json.loads(line)\n",
    "            output = {}\n",
    "            output['text'] = line['sentence']\n",
    "            output['entity_spo_list'] = []\n",
    "            if str(i) not in pred:\n",
    "                outputs.append(output)\n",
    "                continue\n",
    "            cur_pred = pred[str(i)][0][1]\n",
    "            for s_pos, o_pos, predicate in cur_pred:\n",
    "                cur_spo = {}\n",
    "                cur_spo['predicate'] = predicate\n",
    "                cur_spo['subject'] = line['sentence'][s_pos[0]:s_pos[1]+1]\n",
    "                cur_spo['object'] = line['sentence'][o_pos[0]:o_pos[1]+1]\n",
    "                output['entity_spo_list'].append(cur_spo)\n",
    "            outputs.append(output)\n",
    "            \n",
    "    # 写入处理后的结果\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_f:\n",
    "        for d in outputs:\n",
    "            output_f.write(json.dumps(d, ensure_ascii=False) + '\\n')\n",
    "            \n",
    "post_process('./bert_models/re-roberta-zh-1/pred_results.json', '../datasets/my_dataset/eval.json', '../datasets/my_dataset_processed/test_pred.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.735376, Recall: 0.355795, F1: 0.479564\n"
     ]
    }
   ],
   "source": [
    "# 最终整体评估\n",
    "def evaluate_pred(pred_path, true_path):\n",
    "    tot_recall = 0\n",
    "    tot_pred = 0\n",
    "    correct_count = 0\n",
    "    true_results = []\n",
    "    with open(true_path, 'r', encoding='utf8') as true_f:\n",
    "        for line in true_f:\n",
    "            true_results.append(json.loads(line))\n",
    "    with open(pred_path, 'r', encoding='utf8') as pred_f:\n",
    "        for idx, line in enumerate(pred_f):\n",
    "            pred_line = json.loads(line)\n",
    "            true_line = true_results[idx]\n",
    "            tot_recall += len(true_line['spos'])\n",
    "            tot_pred += len(pred_line['entity_spo_list'])\n",
    "            pred_spos = [(item['subject'], item['predicate'], item['object']) for item in pred_line['entity_spo_list']]\n",
    "            true_spos = [(item['s'], item['p'], item['o']) for item in true_line['spos']]\n",
    "            for spo in pred_spos:\n",
    "                if spo in true_spos:\n",
    "                    correct_count += 1\n",
    "    p = correct_count / tot_pred\n",
    "    r = correct_count / tot_recall\n",
    "    f1 = 2*p*r / (p+r)\n",
    "    print(\"Precision: %f, Recall: %f, F1: %f\" % (p, r, f1))\n",
    "    \n",
    "evaluate_pred('../datasets/my_dataset_processed/test_pred.json', '../datasets/my_dataset/eval.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spo关系类别</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>相关文物</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.4688</td>\n",
       "      <td>0.4478</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>所在地</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>所属时代</td>\n",
       "      <td>0.6941</td>\n",
       "      <td>0.4436</td>\n",
       "      <td>0.5413</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>文物尺寸</td>\n",
       "      <td>0.8534</td>\n",
       "      <td>0.4008</td>\n",
       "      <td>0.5454</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>面积大小</td>\n",
       "      <td>0.7778</td>\n",
       "      <td>0.6364</td>\n",
       "      <td>0.7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>发现时间</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.4545</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>保存于</td>\n",
       "      <td>0.9118</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.3924</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>建设时间</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>文保级别</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>相关建筑</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>相关人物</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2308</td>\n",
       "      <td>0.3158</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>相关单位</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>馆藏物品</td>\n",
       "      <td>0.8824</td>\n",
       "      <td>0.3571</td>\n",
       "      <td>0.5084</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>文化类型</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>成立时间</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5455</td>\n",
       "      <td>0.7059</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>文化组成</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>相关文献</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>文物重量</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>馆长</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spo关系类别 Precision  Recall      F1 support\n",
       "0     相关文物    0.4286  0.4688  0.4478      32\n",
       "1      所在地       0.5    0.25  0.3333      48\n",
       "2     所属时代    0.6941  0.4436  0.5413     133\n",
       "3     文物尺寸    0.8534  0.4008  0.5454     247\n",
       "4     面积大小    0.7778  0.6364     0.7      11\n",
       "5     发现时间    0.7143  0.3333  0.4545      30\n",
       "6      保存于    0.9118    0.25  0.3924     124\n",
       "7     建设时间       0.0     0.0     0.0       0\n",
       "8     文保级别      0.75     0.3  0.4286      10\n",
       "9     相关建筑       0.0     0.0     0.0       2\n",
       "10    相关人物       0.5  0.2308  0.3158      13\n",
       "11    相关单位       0.0     0.0     0.0      15\n",
       "12    馆藏物品    0.8824  0.3571  0.5084      42\n",
       "13    文化类型       0.0     0.0     0.0      10\n",
       "14    成立时间       1.0  0.5455  0.7059      11\n",
       "15    文化组成       0.0     0.0     0.0       4\n",
       "16    相关文献       1.0     0.5  0.6667       4\n",
       "17    文物重量       1.0  0.6667     0.8       3\n",
       "18      馆长       0.0     0.0     0.0       3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 根据每个spo的predicate进行评估\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "def spo_report(schema_path, pred_path, true_path):\n",
    "    result = pd.DataFrame([], columns=['spo关系类别', 'Precision', 'Recall', 'F1', 'support'])\n",
    "    report = {}\n",
    "    schema_f = open(schema_path, 'r')\n",
    "    predicate_dict = {}\n",
    "    for l_idx, line in enumerate(schema_f):\n",
    "        tmp = json.loads(line)\n",
    "        predicate_dict[tmp['predicate']] = (tmp['subjtct_type'], tmp['object_type'])\n",
    "    # 对每个类别分别统计\n",
    "    for predicate in predicate_dict:\n",
    "        report[predicate] = {}\n",
    "        report[predicate]['tot_recall'] = 0\n",
    "        report[predicate]['tot_pred'] = 0\n",
    "        report[predicate]['correct_count'] = 0\n",
    "    \n",
    "    # 收集真实的spo\n",
    "    true_results = []\n",
    "    with open(true_path, 'r', encoding='utf8') as true_f:\n",
    "        for line in true_f:\n",
    "            true_results.append(json.loads(line))\n",
    "    \n",
    "    with open(pred_path, 'r', encoding='utf8') as pred_f:\n",
    "        for idx, line in enumerate(pred_f):\n",
    "            pred_line = json.loads(line)\n",
    "            true_line = true_results[idx]\n",
    "            pred_spos = [(item['subject'], item['predicate'], item['object']) for item in pred_line['entity_spo_list']]\n",
    "            true_spos = [(item['s'], item['p'], item['o']) for item in true_line['spos']]\n",
    "            for _,predicate,_ in true_spos:\n",
    "                report[predicate]['tot_recall'] += 1\n",
    "            for spo in pred_spos:\n",
    "                report[spo[1]]['tot_pred'] += 1\n",
    "                if spo in true_spos:\n",
    "                    report[spo[1]]['correct_count'] += 1\n",
    "    \n",
    "    for idx, (k,v) in enumerate(report.items()):\n",
    "        P = round(v['correct_count'] / v['tot_pred'], 4) if v['tot_pred']!=0 else 0.0\n",
    "        R = round(v['correct_count'] / v['tot_recall'],4) if v['tot_recall'] !=0 else 0.0 \n",
    "        F = round((2 * P * R) / (P + R),4) if (P + R)!=0 else 0.0\n",
    "        S = v['tot_recall']\n",
    "        result.loc[idx, :] = [k, P, R, F, S]\n",
    "    return result\n",
    "classification_report = spo_report(\"../datasets/my_dataset/schema.json\", '../datasets/my_dataset_processed/test_pred.json', '../datasets/my_dataset/eval.json')\n",
    "classification_report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
